\section{Introduction}
Natural Language question and answering is a pertinent area of interest in machine learning research, particularly when done in relation to image captioning. However, most machine learning models to encapsulate the complex relations between objects within a picture. Because of this, these image captioning models fail to answer particularly complicated questions about a picture that a person would have no problems with. Despite this, machine learning is the prevailing technique used in visual question answering. We believe that using logic programming to represent human knowledge and reason will allow us to more comprehensively answer natural language questions about that picture. This more closely emulates the manner in which people answer questions about a scene that they can see. We propose that we can apply a logic program to the results of an image captioning model to answer questions about an image.

In this paper, we will apply these concepts to create a program that answers a few questions about an image within the context of autonomous driving. The program receives an image of a road and returns safe actions for the driver to take. It does this by using turning the results of a \emph{dense captioning} model into logic predicates. The program then feeds those predicates into a PROLOG logic program that represents the \emph{commonsense reasoning} that a human would use to evaluate the picture. Finally, it returns safe driving actions based on the results of the logic program. The image captioning model used in this paper is called Dense Relational Captioning\cite{kim2019dense} which is a framework that generates captions the give information about the relations between objects identified in dense image regions. This paper also uses concepts from AUTO-DISCERN\cite{kothawade2021autodiscern} and AQuA\cite{inproceedings} to build the logic program used.

The remainder of the paper is as follows. Section 2 covers relating information and the work this paper is built upon. Section 3 outlines our methodology and lays out the full stack of programs used in the paper. Section 4 displays the results derived from running our program stack over four select images. Section 5 discusses the conclusions derived from our results and contemplates potential future work.